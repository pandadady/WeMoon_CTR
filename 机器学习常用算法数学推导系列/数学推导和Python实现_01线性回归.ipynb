{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于numpy实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_func(X, y, w, b):\n",
    "    train_nums = X.shape[0]\n",
    "    feature_nums = X.shape[1]    \n",
    "    # 模型公式f(x)= w*x + b\n",
    "    y_hat = np.dot(X, w) + b    \n",
    "    # 损失函数\n",
    "    loss = np.sum((y_hat-y)**2)/train_nums    \n",
    "    # 参数的偏导\n",
    "    dw = 2 * np.dot(X.T, (y_hat-y))\n",
    "    db = 2 * np.sum((y_hat-y))   \n",
    "    dw = dw/train_nums\n",
    "    db = db/train_nums\n",
    "    return y_hat, loss, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化权重：w 和 b\n",
    "def initialize_params(dims):\n",
    "    w = np.zeros((dims, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于梯度下降的模型训练过程：\n",
    "def gradient_descent(X, y, learning_rate, epochs):\n",
    "    w, b = initialize_params(X.shape[1])  \n",
    "    loss_list = []  \n",
    "    for i in range(1, epochs):        \n",
    "        # 计算当前预测值、损失和参数偏导\n",
    "        y_hat, loss, dw, db = loss_func(X, y, w, b)  \n",
    "        loss_list.append(loss)      \n",
    "        # 基于梯度下降的参数更新过程\n",
    "        if i%100000==0:\n",
    "            learning_rate=learning_rate*0.8\n",
    "        w += -learning_rate * dw\n",
    "        b += -learning_rate * db        \n",
    "        # 打印迭代次数和损失\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print('epoch %d loss %f' % (i, loss))\n",
    "            print('epoch %d learning_rate %f' % (i, learning_rate))\n",
    "               \n",
    "        # 保存参数\n",
    "        params = {            \n",
    "            'w': w,            \n",
    "            'b': b\n",
    "        }        \n",
    "        \n",
    "        # 保存梯度\n",
    "        grads = {            \n",
    "            'dw': dw,            \n",
    "            'db': db\n",
    "        }    \n",
    "            \n",
    "    return loss_list, loss, params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以上便是线性回归模型的基本实现过程。下面以 sklearn 中的 diabetes 数据集为例进行简单的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train= (397, 10)\n",
      "X_test= (45, 10)\n",
      "y_train= (397, 1)\n",
      "y_test= (45, 1)\n"
     ]
    }
   ],
   "source": [
    "#数据准备：\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "data = diabetes.data\n",
    "target = diabetes.target \n",
    "\n",
    "# 打乱数据\n",
    "X, y = shuffle(data, target, random_state=13)\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# 训练集与测试集的简单划分\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "\n",
    "print('X_train=', X_train.shape)\n",
    "print('X_test=', X_test.shape)\n",
    "print('y_train=', y_train.shape)\n",
    "print('y_test=', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000 loss 2864.567324\n",
      "epoch 10000 learning_rate 0.500000\n",
      "epoch 20000 loss 2858.484056\n",
      "epoch 20000 learning_rate 0.500000\n",
      "epoch 30000 loss 2854.445879\n",
      "epoch 30000 learning_rate 0.500000\n",
      "epoch 40000 loss 2851.765062\n",
      "epoch 40000 learning_rate 0.500000\n",
      "epoch 50000 loss 2849.985346\n",
      "epoch 50000 learning_rate 0.500000\n",
      "epoch 60000 loss 2848.803845\n",
      "epoch 60000 learning_rate 0.500000\n",
      "epoch 70000 loss 2848.019481\n",
      "epoch 70000 learning_rate 0.500000\n",
      "epoch 80000 loss 2847.498765\n",
      "epoch 80000 learning_rate 0.500000\n",
      "epoch 90000 loss 2847.153077\n",
      "epoch 90000 learning_rate 0.500000\n"
     ]
    }
   ],
   "source": [
    "#执行训练：\n",
    "loss_list, loss, params, grads = gradient_descent(X_train, y_train, 0.5, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': array([[  -0.86473399],\n",
      "       [-264.24734793],\n",
      "       [ 485.19146599],\n",
      "       [ 336.05439516],\n",
      "       [-747.05584309],\n",
      "       [ 417.67713741],\n",
      "       [  95.48517373],\n",
      "       [ 219.70910411],\n",
      "       [ 716.12195039],\n",
      "       [  82.48136561]]), 'b': 151.27786131503402}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120.03576734],\n",
       "       [163.39385018],\n",
       "       [134.84083758],\n",
       "       [100.07956453],\n",
       "       [142.88096289]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下面定义一个预测函数对测试集结果进行预测：\n",
    "def predict(X, params):\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "\n",
    "    y_pred = np.dot(X, w) + b    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, params)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#利用 matplotlib 对预测结果和真值进行展示：\n",
    "import matplotlib.pyplot as plt\n",
    "f = X_test.dot(params['w']) + params['b']\n",
    "\n",
    "plt.scatter(range(X_test.shape[0]), y_test)\n",
    "plt.plot(f, color = 'darkorange')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#可见全变量的数据对于线性回归模型的拟合并不好，一来数据本身的分布问题，二来简单的线性模型对于该数据拟合效果差。\n",
    "#当然，我们只是为了演示线性回归模型的基本过程，不要在意效果。\n",
    "#训练过程中损失的下降：\n",
    "plt.plot(loss_list, color = 'blue')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five kold cross validation score is 2768.9837315034647\n",
      "valid score is 3424.3103942523876\n",
      "five kold cross validation score is 2622.6972860346455\n",
      "valid score is 3981.656533144954\n",
      "five kold cross validation score is 2946.642865591133\n",
      "valid score is 2675.130340063957\n",
      "five kold cross validation score is 2975.560410497327\n",
      "valid score is 2549.6305140934614\n",
      "five kold cross validation score is 2989.619578276991\n",
      "valid score is 2484.504069108119\n"
     ]
    }
   ],
   "source": [
    "# 封装一个线性回归类\n",
    "# 笔者对上述过程进行一个简单的 class 封装，其中加入了自定义的交叉验证过程进行训练：\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "\n",
    "class lr_model():\n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "        self.grads = dict()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = load_diabetes().data\n",
    "        target = load_diabetes().target\n",
    "        X, y = shuffle(data, target, random_state=42)\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.reshape((-1, 1))\n",
    "        data = np.concatenate((X, y), axis=1)\n",
    "        return data\n",
    "\n",
    "    def initialize_params(self, dims):\n",
    "        w = np.zeros((dims, 1))\n",
    "        b = 0\n",
    "        return w, b\n",
    "\n",
    "    def loss_func(self, X, y, w, b):\n",
    "        train_nums = X.shape[0]\n",
    "        feature_nums = X.shape[1]\n",
    "        # 模型公式f(x)= w*x + b\n",
    "        y_hat = np.dot(X, w) + b\n",
    "        # 损失函数\n",
    "        loss = np.sum((y_hat - y) ** 2) / train_nums\n",
    "        # 参数的偏导\n",
    "        dw = 2 * np.dot(X.T, (y_hat - y))\n",
    "        db = 2 * np.sum((y_hat - y))\n",
    "        dw = dw / train_nums\n",
    "        db = db / train_nums\n",
    "        return y_hat, loss, dw, db\n",
    "\n",
    "    # 带L2正则项\n",
    "    def loss_func_l2(self, X, y, w, b, alpha=1e-5):\n",
    "        train_nums = X.shape[0]\n",
    "        feature_nums = X.shape[1]\n",
    "        # 模型公式f(x)= w*x + b\n",
    "        y_hat = np.dot(X, w) + b + alpha * np.dot(w.T, w)\n",
    "        # 损失函数\n",
    "        loss = np.sum((y_hat - y) ** 2) / train_nums\n",
    "        # 参数的偏导\n",
    "        dw = 2 * np.dot(X.T, (y_hat - y))\n",
    "        db = 2 * np.sum((y_hat - y))\n",
    "        dw = dw / train_nums\n",
    "        db = db / train_nums\n",
    "        return y_hat, loss, dw, db\n",
    "\n",
    "    # 基于梯度下降的模型训练过程：\n",
    "    def gradient_descent(self, X, y, learning_rate, epochs):\n",
    "        w, b = self.initialize_params(X.shape[1])\n",
    "        loss_list = []\n",
    "        for i in range(1, epochs):\n",
    "            # 计算当前预测值、损失和参数偏导\n",
    "            y_hat, loss, dw, db = self.loss_func(X, y, w, b)\n",
    "            loss_list.append(loss)\n",
    "            # 基于梯度下降的参数更新过程\n",
    "            if i % 10000 == 0:\n",
    "                learning_rate = learning_rate * 0.5\n",
    "            w += -learning_rate * dw\n",
    "            b += -learning_rate * db\n",
    "            # 打印迭代次数和损失\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print('epoch %d loss %f' % (i, loss))\n",
    "                print('epoch %d learning_rate %f' % (i, learning_rate))\n",
    "\n",
    "            # 保存参数\n",
    "            self.params = {\n",
    "                'w': w,\n",
    "                'b': b\n",
    "            }\n",
    "\n",
    "            # 保存梯度\n",
    "            self.grads = {\n",
    "                'dw': dw,\n",
    "                'db': db\n",
    "            }\n",
    "\n",
    "        params, grads= self.params, self.grads\n",
    "        return loss, params, grads\n",
    "\n",
    "    def predict(self, X, params):\n",
    "        w = self.params['w']\n",
    "        b = self.params['b']\n",
    "        y_pred = np.dot(X, w) + b\n",
    "#         print(\"y_pred:\", y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def linear_cross_validation(self, data, k, randomize=True):\n",
    "        if randomize:\n",
    "            data = list(data)\n",
    "            shuffle(data)\n",
    "\n",
    "        slices = [data[i::k] for i in range(k)]\n",
    "        for i in range(k):\n",
    "            validation = slices[i]\n",
    "            train = [dt for s in slices if s is not validation for dt in s]\n",
    "            train = np.array(train)\n",
    "            validation = np.array(validation)\n",
    "            yield train, validation\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = lr_model()\n",
    "    data = lr.prepare_data()\n",
    "\n",
    "    for train, validation in lr.linear_cross_validation(data, 5):\n",
    "        X_train = train[:, :10]\n",
    "        y_train = train[:, -1].reshape((-1, 1))\n",
    "        X_valid = validation[:, :10]\n",
    "        y_valid = validation[:, -1].reshape((-1, 1))\n",
    "\n",
    "        loss5 = []\n",
    "        loss, params, grads = lr.gradient_descent(X_train, y_train, learning_rate=0.3, epochs=10000)\n",
    "        loss5.append(loss)\n",
    "        score = np.mean(loss5)\n",
    "        print('five kold cross validation score is', score)\n",
    "        y_pred = lr.predict(X_valid, params)\n",
    "        valid_score = np.sum(((y_pred - y_valid) ** 2)) / len(X_valid)\n",
    "#         print(\"y_valid:\", y_valid)\n",
    "        print('valid score is', valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
